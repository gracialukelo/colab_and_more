{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ml3IqRdPda5u0LVhnO6E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeebA3nIwj1M"
      },
      "outputs": [],
      "source": [
        "# web_scraper.py\n",
        "\n",
        "# Installationsbefehle (entferne die Kommentarzeichen, wenn du sie ausführen möchtest)\n",
        "# pip install nest_asyncio playwright beautifulsoup4 fpdf\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "from fpdf import FPDF\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Damit wir async in Jupyter/Colab verwenden können\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Logging konfigurieren\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class WebScraper:\n",
        "    def __init__(self, url, output_file=\"extracted_teaser_data\", output_format=\"txt\", output_dir=\"data\"):\n",
        "        \"\"\"\n",
        "        Initialisiert die WebScraper-Klasse.\n",
        "\n",
        "        :param url: URL der zu scrapenden Webseite\n",
        "        :param output_file: Basisname der Ausgabedatei ohne Dateiendung\n",
        "        :param output_format: Format der Ausgabedatei ('txt', 'pdf', 'json', etc.)\n",
        "        :param output_dir: Verzeichnis, in dem die Ausgabedateien gespeichert werden sollen\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.output_file = output_file\n",
        "        self.output_format = output_format.lower()\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)  # Erstelle den Ausgabeordner, falls er nicht existiert\n",
        "\n",
        "    async def fetch_page_content(self):\n",
        "        \"\"\"\n",
        "        Lädt die Webseite und gibt den HTML-Inhalt zurück.\n",
        "\n",
        "        :return: HTML-Inhalt der Webseite\n",
        "        \"\"\"\n",
        "        try:\n",
        "            async with async_playwright() as p:\n",
        "                browser = await p.chromium.launch(headless=True)\n",
        "                page = await browser.new_page()\n",
        "\n",
        "                # Webseite laden\n",
        "                logging.info(f\"Lade Webseite: {self.url}\")\n",
        "                await page.goto(self.url)\n",
        "\n",
        "                # Warte, bis die Seite vollständig geladen ist\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "                # Gesamten HTML-Inhalt der Seite erfassen\n",
        "                content = await page.content()\n",
        "\n",
        "                # Browser schließen\n",
        "                await browser.close()\n",
        "\n",
        "                logging.info(\"Webseiteninhalt erfolgreich abgerufen.\")\n",
        "                return content\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Abrufen der Seite: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Beautiful\n",
        "    def extract_important_attributes(self, html_content):\n",
        "        \"\"\"\n",
        "        Extrahiert wichtige Attribute aus dem HTML-Inhalt.\n",
        "\n",
        "        :param html_content: HTML-Inhalt der Webseite\n",
        "        :return: Liste der extrahierten Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            data = []\n",
        "\n",
        "            # Suche nach spezifischen Elementen, die die relevanten Attribute enthalten\n",
        "            for element in soup.find_all(attrs={\"teaser-title\": True, \"teaser-description\": True, \"teaser-url\": True}):\n",
        "                # Extrahiere die Werte der gewünschten Attribute\n",
        "                teaser_title = element.get(\"teaser-title\")\n",
        "                teaser_description = element.get(\"teaser-description\")\n",
        "                teaser_url = element.get(\"teaser-url\")\n",
        "\n",
        "                # Füge die extrahierten Daten zur Liste hinzu\n",
        "                data.append({\"title\": teaser_title, \"description\": teaser_description, \"url\": teaser_url})\n",
        "\n",
        "            logging.info(\"Wichtige Attribute erfolgreich extrahiert.\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Extrahieren der Attribute: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "\n",
        "    def save_to_txt(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als Textdatei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.txt\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            with file_path.open(\"w\", encoding=\"utf-8\") as file:\n",
        "                for item in data:\n",
        "                    file.write(f\"Title: {item['title']}\\n\")\n",
        "                    file.write(f\"Description: {item['description']}\\n\")\n",
        "                    file.write(f\"URL: {item['url']}\\n\")\n",
        "                    file.write(\"\\n\")  # Leerzeile zwischen den Einträgen\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als TXT: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    def save_to_pdf(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als PDF-Datei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.pdf\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "\n",
        "            # Füge die Unicode-fähige Schriftart hinzu\n",
        "            pdf.add_font(\"DejaVu\", \"\", \"fonts/DejaVuSans.ttf\", uni=True)  # Stelle sicher, dass DejaVuSans.ttf im fonts/ Ordner ist\n",
        "            pdf.set_font(\"DejaVu\", \"\", 12)\n",
        "\n",
        "            for item in data:\n",
        "                title = item['title'] if item['title'] else ''\n",
        "                description = item['description'] if item['description'] else ''\n",
        "                url = item['url'] if item['url'] else ''\n",
        "\n",
        "                pdf.cell(200, 10, txt=f\"Title: {title}\", ln=True)\n",
        "                pdf.multi_cell(0, 10, txt=f\"Description: {description}\")\n",
        "                pdf.cell(200, 10, txt=f\"URL: {url}\", ln=True)\n",
        "                pdf.ln(5)  # Leerzeile zwischen den Einträgen\n",
        "\n",
        "            pdf.output(str(file_path))\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als PDF: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    def save_to_json(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als JSON-Datei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.json\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            with file_path.open(\"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als JSON: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    def save_data_to_file(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten im angegebenen Format.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        if self.output_format == \"txt\":\n",
        "            self.save_to_txt(data)\n",
        "        elif self.output_format == \"pdf\":\n",
        "            self.save_to_pdf(data)\n",
        "        elif self.output_format == \"json\":\n",
        "            self.save_to_json(data)\n",
        "        else:\n",
        "            logging.warning(f\"Unbekanntes Ausgabeformat '{self.output_format}'. Daten werden nicht gespeichert.\")\n",
        "\n",
        "\n",
        "\n",
        "    async def run(self):\n",
        "        \"\"\"\n",
        "        Führt den gesamten Scraping-Prozess aus.\n",
        "        \"\"\"\n",
        "        # HTML-Inhalt abrufen\n",
        "        content = await self.fetch_page_content()\n",
        "        if content is None:\n",
        "            logging.error(\"Keine Daten zum Verarbeiten. Beende das Programm.\")\n",
        "            return\n",
        "\n",
        "        # Wichtige Attribute extrahieren\n",
        "        extracted_data = self.extract_important_attributes(content)\n",
        "        if not extracted_data:\n",
        "            logging.warning(\"Keine Daten extrahiert.\")\n",
        "            return\n",
        "\n",
        "        # Daten speichern\n",
        "        self.save_data_to_file(extracted_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative"
      ],
      "metadata": {
        "id": "HWvZlalRw-7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "\n",
        "# Damit wir async in Jupyter/Colab verwenden können\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def fetch_and_save_pdf(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Webseite laden\n",
        "        await page.goto(url)\n",
        "\n",
        "        # Warte, bis die Seite vollständig geladen ist\n",
        "        await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "        # Erstelle einen gültigen Dateinamen aus der URL\n",
        "        filename = url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "        filename = f\"{filename}.pdf\"\n",
        "        filepath = os.path.join(os.getcwd(), filename)\n",
        "\n",
        "        # Speichere die Seite als PDF\n",
        "        await page.pdf(path=filepath, format=\"A4\")\n",
        "        print(f\"PDF gespeichert: {filepath}\")\n",
        "\n",
        "        # Browser schließen\n",
        "        await browser.close()\n",
        "\n",
        "async def main():\n",
        "    urls = [\n",
        "        #\"http://regelwerke.vbg.de/vbg_gese/nagg/nagg_0_.html\",\n",
        "        \"http://regelwerke.vbg.de/vbg_gese/narbgg/narbgg_0_.html\",\n",
        "        \"http://regelwerke.vbg.de/vbg_gese/narbschg/narbschg_0_.html\",\n",
        "        \"http://regelwerke.vbg.de/vbg_gese/narbzg/narbzg_0_.html\",\n",
        "        \"http://regelwerke.vbg.de/vbg_gese/nasig/nasig_0_.html\",\n",
        "        \"http://regelwerke.vbg.de/vbg_gese/naueg/naueg_0_.html\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Erstelle eine Liste von Aufgaben\n",
        "    tasks = [fetch_and_save_pdf(url) for url in urls]\n",
        "\n",
        "    # Führe die Aufgaben gleichzeitig aus\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "# Führe das Hauptprogramm aus\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "ioFBWix_xE8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}